import logging
import time

from litellm import completion as litellm_completion
from palimpzest.constants import (
    MODEL_CARDS,
    NAIVE_EST_FILTER_SELECTIVITY,
    NAIVE_EST_NUM_INPUT_TOKENS,
    Cardinality,
    Model,
    PromptStrategy,
)
from palimpzest.core.elements.records import DataRecord
from palimpzest.core.models import GenerationStats, OperatorCostEstimates
from palimpzest.query.generators.generators import Generator
from palimpzest.query.operators.filter import FilterOp
from palimpzest.query.operators.logical import FilteredScan
from palimpzest.query.optimizer.primitives import LogicalExpression, PhysicalExpression
from palimpzest.query.optimizer.rules import ImplementationRule
from palimpzest.utils.model_helpers import use_reasoning_prompt
from pydantic import BaseModel
from pydantic.fields import FieldInfo

from ap_picker.datasets.moma.data_reader import RelationalDbReader
from ap_picker.datasets.moma.dataset import MomaDatasetItemType
from ap_picker.datasets.moma.items.relational_db_item import MomaDatasetItemRelationalDb

logger = logging.getLogger(__name__)


class QueryResponse(BaseModel):
    # Sql query generated by the LLM based on the natural language filter condition and database schema
    query: str


class MomaHybridFilter(FilterOp):
    """
    A hybrid filter that uses SQL filtering for relational databases
    and LLM-based semantic filtering for other dataset types.
    """

    def __init__(
        self,
        model: Model,
        prompt_strategy: PromptStrategy = PromptStrategy.FILTER,
        reasoning_effort: str | None = None,
        *args,
        **kwargs,
    ):
        super().__init__(*args, **kwargs)
        self.model = model
        self.prompt_strategy = prompt_strategy
        self.reasoning_effort = reasoning_effort
        if model is not None:
            self.generator = Generator(
                model,
                prompt_strategy,
                reasoning_effort,
                self.api_base,
                Cardinality.ONE_TO_ONE,
                self.desc,
                self.verbose
            )

    def get_id_params(self):
        id_params = super().get_id_params()
        id_params = {
            "model": None if self.model is None else self.model.value,
            "prompt_strategy": None if self.prompt_strategy is None else self.prompt_strategy.value,
            "reasoning_effort": self.reasoning_effort,
            **id_params,
        }
        return id_params

    def get_op_params(self):
        op_params = super().get_op_params()
        op_params = {
            "model": self.model,
            "prompt_strategy": self.prompt_strategy,
            "reasoning_effort": self.reasoning_effort,
            **op_params,
        }
        return op_params

    def get_model_name(self):
        return None if self.model is None else self.model.value

    def naive_cost_estimates(self, source_op_cost_estimates: OperatorCostEstimates):
        # For hybrid filter, we estimate costs assuming mix of SQL and LLM usage
        # Use a weighted average approach

        # LLM filter cost estimation
        est_num_input_tokens = NAIVE_EST_NUM_INPUT_TOKENS
        if self.is_image_op():
            est_num_input_tokens = 765 / 10

        est_num_output_tokens = 1.25

        llm_time_per_record = (
            MODEL_CARDS[self.model.value]["seconds_per_output_token"] *
            est_num_output_tokens
        )

        usd_per_input_token = (
            MODEL_CARDS[self.model.value]["usd_per_audio_input_token"]
            if self.is_audio_op()
            else MODEL_CARDS[self.model.value]["usd_per_input_token"]
        )
        llm_cost_per_record = (
            usd_per_input_token * est_num_input_tokens
            + MODEL_CARDS[self.model.value]["usd_per_output_token"] *
            est_num_output_tokens
        )

        # SQL filter cost estimation (very fast, no cost)
        sql_time_per_record = 0.001
        sql_cost_per_record = 0.0

        # Assume 50/50 mix for cost estimation
        avg_time_per_record = (llm_time_per_record + sql_time_per_record) / 2
        avg_cost_per_record = (llm_cost_per_record + sql_cost_per_record) / 2

        # estimate output cardinality using a constant assumption of the filter selectivity
        selectivity = NAIVE_EST_FILTER_SELECTIVITY
        cardinality = selectivity * source_op_cost_estimates.cardinality

        # estimate quality, sql quality is 1
        quality = ((MODEL_CARDS[self.model.value]
                   ["overall"] / 100.0 + 1.0) / 2.0)

        return OperatorCostEstimates(
            cardinality=cardinality,
            time_per_record=avg_time_per_record,
            cost_per_record=avg_cost_per_record,
            quality=quality,
        )

    def _sql_filter(self, candidate: DataRecord) -> tuple[dict[str, bool], GenerationStats]:
        """Apply SQL-based filtering for relational databases."""
        start_time = time.time()

        if self.filter_obj.filter_condition is None:
            raise ValueError(
                "Expected filter condition for SQL filtering, got None")

        record = candidate.to_dict()
        db_name = record.get("metadata", {}).get("names")[0]
        assert db_name is not None, "Expected database name in metadata for SQL filtering"

        data_reader = RelationalDbReader(db_name)
        # TODO: Get schema of db
        schema = data_reader.get_schema()

        # TODO: From the NL filter condition, generate SQL query
        filter_condition = self.filter_obj.filter_condition.lower()

        response = litellm_completion(
            model=self.model.value,
            response_format=QueryResponse,
            messages=[{"role": "user", "content": f"Convert the following natural language filter condition into a SQL query based on the following database schema: {schema}. \n\n Filter condition: {filter_condition}. Use Fully qualified table name, with their schema"}]
        )

        res = response.choices[0].message.content
        query = QueryResponse.model_validate_json(res).query

        # TODO: Execute the SQL query against the database
        data = data_reader.read_stream(query=query)
        passed = any(data)

        if self.verbose:
            print(f"[SQL Filter] {self.filter_obj.get_filter_str()}: {passed}")

        generation_stats = GenerationStats(
            fn_call_duration_secs=time.time() - start_time
        )

        return {"passed_operator": passed}, generation_stats

    def _llm_filter(self, candidate: DataRecord) -> tuple[dict[str, bool], GenerationStats]:
        """Apply LLM-based semantic filtering."""
        # get the set of input fields to use for the filter operation
        input_fields = self.get_input_fields()

        # construct kwargs for generation
        gen_kwargs = {
            "project_cols": input_fields,
            "filter_condition": self.filter_obj.filter_condition
        }

        # generate output
        fields = {
            "passed_operator": FieldInfo(
                annotation=bool,
                description="Whether the record passed the filter operation"
            )
        }
        field_answers, _, generation_stats, _ = self.generator(
            candidate, fields, **gen_kwargs
        )

        if self.verbose:
            print(
                f"[LLM Filter] {self.filter_obj.get_filter_str()}: {field_answers.get('passed_operator')}")

        return field_answers, generation_stats

    def filter(self, candidate: DataRecord) -> tuple[dict[str, bool], GenerationStats]:
        """
        Apply filtering based on the dataset type:
        - Use SQL filtering for relational databases
        - Use LLM filtering for file-based datasets
        """
        record_dict = candidate.to_dict()
        dataset_type = record_dict.get("type")
        assert isinstance(
            dataset_type, MomaDatasetItemType), f"Expected dataset type to be MomaDatasetItemType, got {type(dataset_type)}"

        if self.verbose:
            print(f"[HybridFilter] Dataset type: {dataset_type}")

        match dataset_type:
            case MomaDatasetItemType.RELATIONAL_DB:
                return self._sql_filter(candidate)
            case MomaDatasetItemType.FILE_DATASET:
                return self._llm_filter(candidate)
            case _:
                raise ValueError(f"Unsupported dataset type: {dataset_type}")


class HybridFilterRule(ImplementationRule):
    """
    Substitute a logical FilteredScan with a hybrid filter that can handle
    both relational databases (with SQL) and other datasets (with LLM).
    """

    @classmethod
    def matches_pattern(cls, logical_expression: LogicalExpression) -> bool:
        """
        This rule matches FilteredScan operations on MomaDataset
        (which have a 'type' field indicating the dataset type).
        """
        logical_op = logical_expression.operator

        # Check if it's a FilteredScan with semantic filter (no filter_fn)
        if not isinstance(logical_op, FilteredScan) or logical_op.filter.filter_fn is not None:
            return False

        # Check if the input schema has a 'type' field (MomaDataset characteristic)
        has_type_field = any(key.endswith('.type')
                             for key in logical_expression.input_fields.keys())

        is_match = has_type_field

        logger.debug(
            f"HybridFilterRule matches_pattern: {is_match} for {logical_expression}"
        )
        return is_match

    @classmethod
    def substitute(
        cls,
        logical_expression: LogicalExpression,
        **runtime_kwargs
    ) -> set[PhysicalExpression]:
        """Create physical plans with HybridFilter for each available model."""
        logger.debug(f"Substituting HybridFilterRule for {logical_expression}")

        # create variable physical operator kwargs for each model
        models = [
            model for model in runtime_kwargs["available_models"]
            if cls._model_matches_input(model, logical_expression)
        ]

        variable_op_kwargs = []
        for model in models:
            reasoning = use_reasoning_prompt(
                runtime_kwargs["reasoning_effort"])
            prompt_strategy = (
                PromptStrategy.FILTER if reasoning
                else PromptStrategy.FILTER_NO_REASONING
            )
            variable_op_kwargs.append({
                "model": model,
                "prompt_strategy": prompt_strategy,
                "reasoning_effort": runtime_kwargs["reasoning_effort"],
            })

        return cls._perform_substitution(
            logical_expression,
            MomaHybridFilter,
            runtime_kwargs,
            variable_op_kwargs
        )
