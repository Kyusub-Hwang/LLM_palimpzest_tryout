"""
Dataset Expand Operator - Converts dataset-level meta-records to row-level records.

This operator implements the expand() operation as a proper Palimpzest ConvertOp,
providing better integration with the optimization framework and query planning.
"""

import logging
import time
from typing import Any, Dict, List, Optional

from litellm import completion as litellm_completion
from palimpzest.constants import (
    NAIVE_EST_ONE_TO_MANY_SELECTIVITY,
    Cardinality,
)
from palimpzest.core.elements.records import DataRecord
from palimpzest.core.models import GenerationStats, OperatorCostEstimates
from palimpzest.query.operators.convert import NonLLMConvert
from pydantic import BaseModel
from pydantic.fields import FieldInfo

from ap_picker.datasets.moma.data_reader import RelationalDbReader
from ap_picker.datasets.moma.items import MomaDatasetItemType

logger = logging.getLogger(__name__)


class QueryResponse(BaseModel):
    """SQL query generated by the LLM for filtering."""
    query: str


class DatasetExpandConvert(NonLLMConvert):
    """
    Convert operation that expands dataset-level meta-records into row-level records.

    This is a ONE_TO_MANY conversion where each dataset meta-record is expanded
    into multiple individual records (rows from DB tables, entries from files, etc.).

    Supports lazy filtering:
    - Stores filter conditions that are applied during expansion
    - For relational DBs: Generates SQL WHERE clause for efficient filtering
    - For file datasets: Could filter during read (not yet implemented)
    """

    def __init__(
        self,
        output_schema: type[BaseModel],
        input_schema: type[BaseModel] | None = None,
        lazy_filters: Optional[List[str]] = None,
        *args,
        **kwargs
    ):
        """
        Initialize DatasetExpandConvert operator.

        Args:
            output_schema: Pydantic schema for output records
            input_schema: Pydantic schema for input records
            lazy_filters: List of filter conditions to apply during expansion (combined with AND)
            *args, **kwargs: Passed to parent NonLLMConvert
        """
        # Set cardinality to ONE_TO_MANY since each dataset expands to multiple records
        super().__init__(
            output_schema=output_schema,
            input_schema=input_schema,
            cardinality=Cardinality.ONE_TO_MANY,
            udf=self._create_expand_udf(lazy_filters),
            desc=f"Expand datasets to records{' with filters' if lazy_filters else ''}",
            *args,
            **kwargs
        )
        self.lazy_filters = lazy_filters or []

    def get_id_params(self):
        """Add lazy_filters to operator ID for caching."""
        id_params = super().get_id_params()
        id_params["lazy_filters"] = self.lazy_filters
        return id_params

    def get_op_params(self):
        """Add lazy_filters to operator parameters."""
        op_params = super().get_op_params()
        op_params["lazy_filters"] = self.lazy_filters
        return op_params

    def __str__(self):
        """String representation showing filters."""
        s = super().__str__()
        if self.lazy_filters:
            s += f"    Lazy Filters: {self.lazy_filters}\n"
        return s

    def naive_cost_estimates(self, source_op_cost_estimates: OperatorCostEstimates) -> OperatorCostEstimates:
        """
        Estimate costs for dataset expansion.

        Expansion is relatively expensive because:
        - Relational DBs: Must execute SQL queries and stream results
        - File datasets: Must read and parse files
        - With filters: Additional cost for LLM to generate SQL
        """
        # Base expansion time depends on whether we have filters
        if self.lazy_filters:
            # With filters: Additional LLM call to generate SQL query (~2-3 seconds per dataset)
            time_per_record = 2.5
            # LLM cost for SQL generation (small, structured output)
            cost_per_record = 0.01
        else:
            # Without filters: Just database read time (~1 second per dataset)
            time_per_record = 1.0
            cost_per_record = 0.0

        # Estimate output cardinality
        # Average dataset might have 100-1000 records
        # With filters, expect ~20% selectivity
        avg_records_per_dataset = 500
        filter_selectivity = 0.2 if self.lazy_filters else 1.0
        expected_records = avg_records_per_dataset * filter_selectivity

        # Total cardinality: number of input datasets * records per dataset
        cardinality = source_op_cost_estimates.cardinality * expected_records

        # Distribute time across all generated records
        time_per_output_record = time_per_record / max(expected_records, 1)
        cost_per_output_record = cost_per_record / max(expected_records, 1)

        # Quality is high (deterministic expansion, no LLM hallucination for data)
        quality = 0.99

        return OperatorCostEstimates(
            cardinality=cardinality,
            time_per_record=time_per_output_record,
            cost_per_record=cost_per_output_record,
            quality=quality,
        )

    @staticmethod
    def _create_expand_udf(lazy_filters: Optional[List[str]]):
        """
        Create the UDF function that performs the expansion.

        This is a closure that captures the lazy_filters so they can be
        applied during expansion.
        """
        def expand_udf(meta_record: dict) -> List[dict]:
            """
            Expand a single dataset meta-record into multiple records.

            Args:
                meta_record: Dict with keys: source_dataset_id, source_dataset_desc,
                            source_dataset_type, record_data (for already-expanded records)
                            OR id, description, type, content, metadata (for meta-records)

            Returns:
                List of dicts with envelope schema for each record
            """
            # Check if this is already an expanded record (has 'source_dataset_id')
            # If so, just pass it through (no re-expansion needed)
            if "source_dataset_id" in meta_record:
                return [meta_record]

            # Extract dataset metadata
            dataset_id = meta_record.get("id", "unknown")
            description = meta_record.get("description", "")
            dataset_type_raw = meta_record.get("type")

            # Convert enum to string if needed
            if isinstance(dataset_type_raw, MomaDatasetItemType):
                dataset_type = dataset_type_raw
            elif isinstance(dataset_type_raw, str):
                try:
                    dataset_type = MomaDatasetItemType(dataset_type_raw)
                except (ValueError, KeyError):
                    logger.warning(
                        f"Invalid dataset type string '{dataset_type_raw}' for {dataset_id}, skipping")
                    return []
            else:
                logger.warning(
                    f"Invalid dataset type for {dataset_id}, skipping")
                return []

            results = []
            try:
                match dataset_type:
                    case MomaDatasetItemType.RELATIONAL_DB:
                        results = _expand_relational_db(
                            meta_record, dataset_id, description, dataset_type, lazy_filters
                        )

                    case MomaDatasetItemType.FILE_DATASET:
                        logger.warning(
                            f"File dataset expansion not yet fully implemented for {dataset_id}")
                        return []

                    case _:
                        logger.warning(
                            f"Unsupported dataset type {dataset_type} for {dataset_id}")
                        return []

            except Exception as e:
                logger.error(f"Failed to expand dataset {dataset_id}: {e}")

            return results

        return expand_udf


def _expand_relational_db(
    meta_record: dict,
    dataset_id: str,
    description: str,
    dataset_type: MomaDatasetItemType,
    lazy_filters: Optional[List[str]] = None
) -> List[dict]:
    """
    Expand a relational database dataset into individual records.

    Args:
        meta_record: Dataset meta-record
        dataset_id: Dataset ID
        description: Dataset description
        dataset_type: Dataset type enum
        lazy_filters: Optional filters to apply via SQL WHERE

    Returns:
        List of envelope-wrapped records
    """
    results = []

    # Get database connection info from metadata
    metadata = meta_record.get("metadata", {})
    db_name = metadata.get("names", [None])[0]
    if not db_name:
        logger.warning(f"No database name for {dataset_id}")
        return []

    reader = RelationalDbReader(db_name)

    # Apply lazy filters if provided (generates SQL WHERE clause)
    if lazy_filters and len(lazy_filters) > 0:
        try:
            schema = reader.get_schema()

            # Generate SQL query with LLM
            response = litellm_completion(
                model="ollama/llama3.1",  # TODO: Make configurable
                response_format=QueryResponse,
                messages=[{
                    "role": "user",
                    "content": f"""Generate a SQL query to select records matching ALL of these conditions (combined with AND).

Database schema: {schema}
Filter conditions (must satisfy ALL):
{chr(10).join(f'  - {cond}' for cond in lazy_filters)}

Requirements:
1. SELECT all columns from relevant tables
2. Use WHERE clause with AND to combine all filter conditions
3. Use fully qualified table names with schema
4. Return all matching records (no LIMIT)

Example with multiple filters:
  Filters: ["about algebra", "difficulty is basic"]
  Query: "SELECT * FROM schema.questions WHERE question ILIKE '%algebra%' AND difficulty = 'basic'"
"""
                }]
            )

            query_text = QueryResponse.model_validate_json(
                response.choices[0].message.content).query

            logger.info(
                f"[DatasetExpandConvert] Applying {len(lazy_filters)} filter(s) to {dataset_id}")
            logger.debug(f"  Filters: {lazy_filters}")
            logger.debug(f"  SQL: {query_text}")

            records_iterator = reader.read_stream(query=query_text)

        except Exception as e:
            logger.warning(
                f"Failed to generate/execute filtered query for {dataset_id}: {e}")
            logger.info("  Falling back to unfiltered expansion")
            records_iterator = reader.read_stream()
    else:
        # No filters - read all records
        records_iterator = reader.read_stream()

    # Wrap each record in envelope schema
    for record in records_iterator:
        envelope = {
            "source_dataset_id": dataset_id,
            "source_dataset_desc": description,
            "source_dataset_type": str(dataset_type.value),
            "record_data": record,
        }
        results.append(envelope)

    return results
