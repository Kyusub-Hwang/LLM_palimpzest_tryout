import logging
import time
from typing import Any, Dict, Hashable, cast

from litellm import Choices, ModelResponse
from litellm import completion as litellm_completion
from orjson import dumps
from palimpzest.constants import (
    MODEL_CARDS,
    NAIVE_EST_FILTER_SELECTIVITY,
    NAIVE_EST_NUM_INPUT_TOKENS,
    Cardinality,
    Model,
    PromptStrategy,
)
from palimpzest.core.elements.records import DataRecord
from palimpzest.core.models import GenerationStats
from palimpzest.query.operators.convert import LLMConvert
from palimpzest.query.operators.logical import ConvertScan
from palimpzest.query.operators.physical import PhysicalOperator
from palimpzest.query.optimizer.primitives import (
    Expression,
    Group,
    LogicalExpression,
    PhysicalExpression,
)
from palimpzest.query.optimizer.rules import ImplementationRule
from palimpzest.utils.model_helpers import use_reasoning_prompt
from pydantic import BaseModel
from pydantic.fields import FieldInfo

from ap_picker.datasets.moma.data_reader import DataReaderFactory, RelationalDbReader
from ap_picker.datasets.moma.data_reader.data_reader import DataReader
from ap_picker.datasets.moma.items.item import MomaDatasetItemType

logger = logging.getLogger(__name__)


class QueryResponse(BaseModel):
    """SQL query generated by the LLM."""
    query: str


class LLMMetaDatasetExpansion(LLMConvert):

    def convert(self, candidate: DataRecord, fields: dict[str, FieldInfo]) -> tuple[dict[str, list], GenerationStats]:

        # get the set of input fields to use for the convert operation
        input_fields = self.get_input_fields()

        # NOTE: This is kind of a hack - The filter condition is pulled from the map desc
        # TODO: Try to refactor into a filter/map approach
        filter_condition = self.desc
        assert filter_condition is not None, "Filter condition (desc) is required for LLMMetaDatasetExpansion"
        record = candidate.to_dict()
        try:
            reader, item_type = DataReaderFactory.create(record)
        except ValueError as e:
            logger.warning(
                f"Failed to create data reader for candidate {candidate}: {e}")
            return self.empty_return_data(), GenerationStats()

        if self.verbose:
            logger.info(f"Filtering dataset: {candidate.get('id', 'unknown')}")

        match item_type:
            case MomaDatasetItemType.RELATIONAL_DB:
                return self._sql_filter(record, filter_condition, reader)
            case _:
                logger.warning(f"Unsupported dataset type: {item_type}")
                return self.empty_return_data(), GenerationStats()

    def _sql_filter(self, record: Dict[Hashable, Any], filter_condition: str, reader: DataReader) -> tuple[dict[str, Any], GenerationStats]:
        """
        Use SQL to check if database contains ANY records matching the filter.
        More efficient than loading all records (uses LIMIT 1).
        """
        start_time = time.time()
        schema = reader.get_schema()

        # NOTE: We can use cast here because Streaming is disabled, so the response will be a ModelReponse, not a stream
        response = cast(ModelResponse, litellm_completion(
            model=self.model.value,
            response_format=QueryResponse,
            stream=False,
            messages=[{
                "role": "user",
                "content": f"""
                Convert this natural language filter condition into a SQL query that fetches data from a postgres database.

                Database schema: {schema}
                Filter condition: {filter_condition}

                Requirements:
                1. Use fully qualified table names with schema
                2. Return empty result if no matches
                """
            }]
        ))
        raw_payload = cast(Choices, response.choices[0]).message.content
        if raw_payload is None:
            logger.error("LLM did not return any content")
            return {"passed_operator": False}, GenerationStats(fn_call_duration_secs=time.time() - start_time)

        query = QueryResponse.model_validate_json(raw_payload).query

        try:
            rows = reader.read_stream(query=query)
            data = []
            for i, row in enumerate(rows):
                logger.debug(f"SQL query returned row: {row}")
                data.append(dumps(row).decode("utf-8"))
                if i >= 5:  # limit to 5 rows for efficiency
                    break
            # return_data = {
            #     "source_dataset_id": [candidate.get("id")],
            #     "source_dataset_desc": [candidate.get("description")],
            #     "source_dataset_type": [candidate.get("type")],
            #     "record_data": data,
            # }
            return_data = {
                "source_dataset_id": [record.get("id")],
                "source_dataset_desc": [record.get("description")],
                "source_dataset_type": [record.get("type")],
                "record_data": data,
            }

        except Exception as e:
            logger.error(f"SQL query failed: {e}")
            return_data = self.empty_return_data()
        return return_data, GenerationStats(fn_call_duration_secs=time.time() - start_time)

    def empty_return_data(self):
        return {
            "source_dataset_id": [],
            "source_dataset_desc": [],
            "source_dataset_type": [],
            "record_data": [],
        }


class LLMMetaDatasetExpansionRule(ImplementationRule):
    """
    Substitute a logical expression for a ConvertScan with a bonded convert physical implementation.
    """
    @classmethod
    def _is_meta_dataset(cls, logical_expression: LogicalExpression) -> bool:
        """
        Returns true if this dataste is a meta dataset, a dataset containing other datasets as records
        """
        input_field_keys = set(logical_expression.input_fields.keys())
        # Extract field names without the dataset prefix
        field_names = {key.split('.')[-1] for key in input_field_keys}
        return (
            {'id', 'type', 'content', 'metadata'}.issubset(field_names) and
            'source_dataset_id' not in field_names
        )

    @classmethod
    def matches_pattern(cls, logical_expression: LogicalExpression) -> bool:
        is_match = isinstance(logical_expression.operator,
                              ConvertScan) and logical_expression.operator.udf is None and cls._is_meta_dataset(logical_expression)
        logger.debug(
            f"LLMMetaDatasetExpansionRule matches_pattern: {is_match} for {logical_expression}")
        return is_match

    @classmethod
    def substitute(cls, logical_expression: LogicalExpression, **runtime_kwargs) -> set[PhysicalExpression]:  # type: ignore # noqa
        logger.debug(
            f"Substituting LLMMetaDatasetExpansionRule for {logical_expression}")

        # create variable physical operator kwargs for each model which can implement this logical_expression
        models = [model for model in runtime_kwargs["available_models"]
                  if cls._model_matches_input(model, logical_expression)]
        variable_op_kwargs = []
        for model in models:
            reasoning_prompt_strategy = use_reasoning_prompt(
                runtime_kwargs["reasoning_effort"])
            prompt_strategy = PromptStrategy.MAP if reasoning_prompt_strategy else PromptStrategy.MAP_NO_REASONING
            variable_op_kwargs.append(
                {
                    "model": model,
                    "prompt_strategy": prompt_strategy,
                    "reasoning_effort": runtime_kwargs["reasoning_effort"],
                }
            )

        return cls._perform_substitution(logical_expression, LLMMetaDatasetExpansion, runtime_kwargs, variable_op_kwargs)
